---
author: Charitarth Chugh 
---
# Feedforward Neural Networks With PyTorch


## Introduction to Neural Networks

Neural Networks are a type of machine learning algorithm that consist of neurons that are inspired from the neural connections in the brain. These neurons are connected by edges, modeling the synapse and forming a directed, weighted graph.
```{dot}
digraph NeuralNetwork {

    rankdir=LR
    splines=line
        
        node [fixedsize=true, label=""];

        subgraph cluster_0 {
        color=white;
        node [style=solid,color=blue4, shape=circle];
        x1 x2 x3 x4;
        label = "layer 1 (Input layer)";
    }

    subgraph cluster_1 {
        color=white;
        node [style=solid,color=red2, shape=circle];
        a12 a22 a32;
        label = "layer 2 (hidden layer)";
    }

    subgraph cluster_2 {
        color=white;
        node [style=solid,color=seagreen2, shape=circle];
        l21,l22;
        label="layer 3 (hidden layer)";
    }
    subgraph cluster_3 {
        color=white;
        node [style=solid,color=yellow, shape=circle];
        0;
        label="layer 4 (output layer)";
    }
        x1 -> a12;
        x1 -> a22;
        x1 -> a32;
        x2 -> a12;
        x2 -> a22;
        x2 -> a32;
        x3 -> a12;
        x3 -> a22;
        x3 -> a32;
        x4 -> a12;
        x4 -> a22;
        x4 -> a32;
    

        a12 -> l21
        a22 -> l21
        a32 -> l21
        a12 -> l22
        a22 -> l22
        a32 -> l22
        
        l21 -> 0
        l22 -> 0
}
```

The use of neural networks are quite vast, as they form the core of 'deep learning'. Applicatons can be found image and speech recognition, natural language processing, predictive analytics, robotics, and more.


## Core Concepts

### Neurons & Layers
#### Neurons
Neurons are individually a mathematical operation that can be modelled using the following function:

$$
\begin{equation}
y=\phi (\sum_{j=0}^{m}W_{j}x_j) 
\label{basic_neuron}\tag{1}
\end{equation}
$$

Where:  
- $\phi$ is the [activation function](#activation-functions)  
- $x$ is the input signals to the neuron.  
- $W$ is the weight matrix.  
 
> Note: Usually, the $x_0$ input is assigned the value +1, which makes it a bias input. This leaves only m actual inputs to the neuron, from $x_1$ to $x_m$

Fundementally this is not too different from the plain linear equation $y=Ax+b$, as $x_0$ is the bias/intercept term and $A$ is analogous to $W$ as the weight.


#### Layers
Layers are a way of organizing neurons. Neurons in one layer cannot connect to each other, but can connect to all the neurons in the layers that come before and after the current layer.
As a result, equation $\eqref{basic_neuron}$ changes slightly to become:
$$
y_{k}=\varphi\left( \sum_{j=0}^{m} w_{k j} x_{j} \right) 
\label{layer_neuron}\tag{2}
$$

Where $k$ represents the $k^{th}$ neuron in a layer.  

Here is a visualization of it:  
![Neuron in a Layer](https://upload.wikimedia.org/wikipedia/commons/b/b0/Artificial_neuron.png)

<a href="https://creativecommons.org/licenses/by-sa/2.0" title="Creative Commons Attribution-Share Alike 2.0">CC BY-SA 2.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=125222">Link</a>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ls1dJqZtI7w?si=WZfBEA_uR0RGOEWI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

> Terminology: Hidden layers are the layers between the input and output layers

### Activation Functions
Activation functions tranform models from a linear operation to a non-linear operations. Without this, there is no point of having multiple nuerons connect to each other since effectively the network can be represented as one big linear function. By introducing non-lineararity, we are able to model more complex functions. 

#### ReLU
The Rectified Linear Unit is the simplest form of nonlinearity we can add to a network and is represented by the following function: 
$$
f(x) = \begin{cases}
    x & \text{if } x > 0, \\
    0 & \text{otherwise}.
\end{cases} \qquad\qquad
$$

#### Sigmoid
The sigmoid function is another common non-linear function that can be modelled by the following function:
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$


### Loss Function & Metrics
Now that a model can be defined, it is essential to be able to train it in order to learn $W$. In order to do this, optimization algorithms such as [Gradient Descent](#gradient-descent), which rely on finding the loss (otherwise known as the cost). 

#### Loss
The loss function quantifies how well a model's predictions match the actual target values. It calculates the inconsistency between predicted and actual values, providing a measure of error. The goal during training is to minimize this loss function, thereby improving the model's performance.

Common loss functions vary depending on the task at hand. For regression problems, Mean Squared Error (MSE) or Mean Absolute Error (MAE) are often used. In classification tasks, Cross-Entropy Loss, also known as Log Loss, is frequently employed.

> Loss (specifically validation loss) should rarely be zero. That indicates gross overfitting (the model would perform well on sample data, but won't do well in the real world).

#### Metrics (a sidenote)
Metrics are evaluation criteria used to assess the performance of a model. Unlike the loss function, which is utilized during training to adjust the model's parameters, metrics are employed after training to gauge how well the model performs on unseen data. Metrics provide insights into different aspects of model performance, such as accuracy, precision, recall, F1-score, etc.

Choosing appropriate metrics depends on the specific task and requirements. For instance, in a binary classification problem, accuracy might be a primary metric, but if the classes are imbalanced, precision, recall, or F1-score might provide a more comprehensive assessment.


### Gradient Descent, Automatic Differentiation, and Backpropogation 
These form the core of how Neural Networks work. 

#### Gradient Descent

Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively adjusting the parameters of the model. It operates by calculating the gradient of the loss function with respect to the model's parameters and updating the parameters in the opposite direction of the gradient, moving towards the minimum of the loss function.

#### Automatic Differentiation

Automatic Differentiation is a computational technique used to efficiently and accurately evaluate gradients of functions. It automatically breaks down complex functions into elementary operations, allowing the calculation of derivatives with respect to input variables or parameters. This is crucial for training neural networks, as it enables efficient computation of gradients required by optimization algorithms like Gradient Descent.

#### Backpropagation

Backpropagation is a specific algorithm for efficiently computing gradients of the loss function with respect to the parameters of a neural network. It utilizes the chain rule of calculus to propagate gradients backward through the network, starting from the output layer and moving towards the input layer. This allows for efficient calculation of gradients at each layer, enabling gradient-based optimization algorithms to update the model parameters effectively during training. Backpropagation is fundamental to training deep neural networks and is the cornerstone of modern deep learning techniques.

## PyTorch

PyTorch is a machine learning library for tensor computation and deep neural networks.  

### Tensors
Tensors are PyTorch's name for multidimensional arrays and are not related to the terminology used in physics. 
```{python}
# from torch import nn, Tensor
```



## References
1. [d2l.ai](https://d2l.ai)
2. [nnfs.io](https://nnfs.io)
   1. [Layer animation](https://youtu.be/Ls1dJqZtI7w) 
3. The following Wikipedia articles: <!--TODO:MARKDOWN LINKS-->
   1. https://en.wikipedia.org/wiki/Loss_function
   2. https://en.wikipedia.org/wiki/Artificial_neuron
   3. https://en.wikipedia.org/wiki/Neural_network_(machine_learning)